{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_home_dir = \"./hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = hf_home_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"alexandrainst/nst-da\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# random.seed(10)\n",
    "# rands = random.sample(range(len(dataset)), num_samples)\n",
    "# dataset = dataset.select(rands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(\n",
    "    lambda x: not (set(\"0123456789\") & set(x)), input_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    keep_in_memory=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "dataset_vocab = set(vocabs[\"train\"][\"vocab\"][0] + vocabs[\"test\"][\"vocab\"][0])\n",
    "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vocab = dataset_vocab - tokenizer_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "    (\"&\", \"og\"),\n",
    "    (\"\\r\", \" \"),\n",
    "    (\"´\", \"\"),\n",
    "    (\"\\\\\", \"\"),\n",
    "    (\"¨\", \" \"),\n",
    "    (\"Å\", \"AA\"),\n",
    "    (\"Æ\", \"AE\"),\n",
    "    (\"É\", \"E\"),\n",
    "    (\"Ö\", \"OE\"),\n",
    "    (\"Ø\", \"OE\"),\n",
    "    (\"á\", \"a\"),\n",
    "    (\"ä\", \"ae\"),\n",
    "    (\"å\", \"aa\"),\n",
    "    (\"è\", \"e\"),\n",
    "    (\"î\", \"i\"),\n",
    "    (\"ô\", \"oe\"),\n",
    "    (\"ö\", \"oe\"),\n",
    "    (\"ø\", \"oe\"),\n",
    "    (\"ü\", \"y\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(inputs):\n",
    "    for src, dst in replacements:\n",
    "        inputs[\"text\"] = inputs[\"text\"].replace(src, dst)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(cleanup_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "speaker_counts = defaultdict(int)\n",
    "\n",
    "for speaker_id in dataset[\"train\"][\"speaker_id\"]:\n",
    "    speaker_counts[speaker_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(speaker_counts.values(), bins=20)\n",
    "plt.ylabel(\"Speakers\")\n",
    "plt.xlabel(\"Examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_speaker(speaker_id):\n",
    "    return 280 <= speaker_counts[speaker_id] <= 327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].filter(\n",
    "    select_speaker,\n",
    "    input_columns=[\"speaker_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(dataset[\"train\"][\"speaker_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_id_examples = [\n",
    "    (k, v) for (k, v) in list(speaker_counts.items()) if 280 <= v <= 327\n",
    "]\n",
    "speaker_id_examples_sorted = sorted(\n",
    "    speaker_id_examples, key=lambda x: x[1], reverse=True\n",
    ")\n",
    "speaker_id_examples_sorted[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import SpectralMaskEnhancement\n",
    "\n",
    "metricgan_model_name = \"speechbrain/metricgan-plus-voicebank\"\n",
    "\n",
    "\n",
    "enhance_model = SpectralMaskEnhancement.from_hparams(\n",
    "    source=metricgan_model_name,\n",
    "    savedir=os.path.join(\"/tmp\", metricgan_model_name),\n",
    "    run_opts={\"device\": device},\n",
    ")\n",
    "\n",
    "\n",
    "def enhance_audio(waveform):\n",
    "    tensor = torch.tensor(waveform).reshape(1, -1).float()\n",
    "    enhanced = enhance_model.enhance_batch(tensor, lengths=torch.tensor([1.0]))\n",
    "    enhanced = enhanced.squeeze().cpu().numpy()\n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resemble_enhance.enhancer.inference import enhance\n",
    "from IPython.utils import io\n",
    "\n",
    "\n",
    "def mega_enhance_audio(\n",
    "    waveform, sr, device=\"cuda\", nfe=64, solver=\"midpoint\", lambd=0.9, tau=0.95\n",
    "):\n",
    "    tensor = torch.tensor(waveform).float()\n",
    "    with io.capture_output() as _:\n",
    "        enhanced, new_sr = enhance(\n",
    "            tensor, sr, device, nfe=nfe, solver=solver, lambd=lambd, tau=tau\n",
    "        )\n",
    "    enhanced_cpu = enhanced.cpu().numpy()\n",
    "    return enhanced_cpu, new_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "assert sampling_rate == 16000\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id = \"JackismyShephard/nst-da-norm\"\n",
    "\n",
    "# dataset[\"train\"].push_to_hub(\n",
    "#     dataset_id, split=\"train\", commit_message=\"add train split\"\n",
    "# )\n",
    "\n",
    "# dataset[\"test\"].push_to_hub(dataset_id, split=\"test\", commit_message=\"add test split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import DatasetCard, DatasetCardData\n",
    "\n",
    "# # Using the Default Template\n",
    "# card_data = DatasetCardData(\n",
    "#     size_categories=\"100K<n<1M\",\n",
    "#     license=\"cc0-1.0\",\n",
    "#     task_categories=[\"automatic-speech-recognition\", \"text-to-speech\"],\n",
    "#     language=\"da\",\n",
    "#     pretty_name=\"NST-da Normalized\",\n",
    "#     annotations_creators=[\"machine-generated\", \"expert-generated\"],\n",
    "#     language_creators=[\"expert-generated\"],\n",
    "#     multilinguality=\"monolingual\",\n",
    "#     source_datasets=\"extended\",\n",
    "# )\n",
    "# card = DatasetCard.from_template(\n",
    "#     card_data,\n",
    "# )\n",
    "# card.push_to_hub(dataset_id, commit_message=\"update dataset card\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_23_vestjylland = dataset[\"train\"].filter(\n",
    "    lambda x: x == 202, input_columns=[\"speaker_id\"]\n",
    ")[2]\n",
    "female_24_storkoebenhavn = dataset[\"train\"].filter(\n",
    "    lambda x: x == 404, input_columns=[\"speaker_id\"]\n",
    ")[55]\n",
    "female_49_nordjylland = dataset[\"train\"].filter(\n",
    "    lambda x: x == 419, input_columns=[\"speaker_id\"]\n",
    ")[1]\n",
    "male_51_vest_sydsjaelland = dataset[\"train\"].filter(\n",
    "    lambda x: x == 475, input_columns=[\"speaker_id\"]\n",
    ")[1]\n",
    "male_18_vest_sydsjaelland = dataset[\"train\"].filter(\n",
    "    lambda x: x == 83, input_columns=[\"speaker_id\"]\n",
    ")[17]\n",
    "male_31_fyn = dataset[\"train\"].filter(lambda x: x == 496, input_columns=[\"speaker_id\"])[\n",
    "    37\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_22_oestjylland = dataset[\"train\"].filter(\n",
    "    lambda x: x == 301, input_columns=[\"speaker_id\"]\n",
    ")[0]\n",
    "female_24_storkoebenhavn_2 = dataset[\"train\"].filter(\n",
    "    lambda x: x == 404, input_columns=[\"speaker_id\"]\n",
    ")[0]\n",
    "female_44_nordjylland = dataset[\"train\"].filter(\n",
    "    lambda x: x == 517, input_columns=[\"speaker_id\"]\n",
    ")[0]\n",
    "\n",
    "male_18_vest_syd_sjaelland = dataset[\"train\"].filter(\n",
    "    lambda x: x == 83, input_columns=[\"speaker_id\"]\n",
    ")[2]\n",
    "male_31_fyn_2 = dataset[\"train\"].filter(\n",
    "    lambda x: x == 496, input_columns=[\"speaker_id\"]\n",
    ")[8]\n",
    "male_55_storkoebenhavn = dataset[\"train\"].filter(\n",
    "    lambda x: x == 43, input_columns=[\"speaker_id\"]\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_list = [\n",
    "    (female_23_vestjylland, \"female_23_vestjylland.npy\"),\n",
    "    (female_24_storkoebenhavn, \"female_24_storkoebenhavn.npy\"),\n",
    "    (female_49_nordjylland, \"female_49_nordjylland.npy\"),\n",
    "    (male_51_vest_sydsjaelland, \"male_51_vest_sydsjaelland.npy\"),\n",
    "    (male_18_vest_sydsjaelland, \"male_18_vest_sydsjaelland.npy\"),\n",
    "    (male_31_fyn, \"male_31_fyn.npy\"),\n",
    "]\n",
    "\n",
    "speaker_embeddings_list = [\n",
    "    (create_speaker_embedding(enhance_audio(speaker[\"audio\"][\"array\"])), file_name)\n",
    "    for (speaker, file_name) in speaker_list\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_list_2 = [\n",
    "    (female_22_oestjylland, \"female_22_oestjylland.npy\"),\n",
    "    (female_24_storkoebenhavn_2, \"female_24_storkoebenhavn.npy\"),\n",
    "    (female_44_nordjylland, \"female_44_nordjylland.npy\"),\n",
    "    (male_18_vest_syd_sjaelland, \"male_18_vest_syd_sjaelland.npy\"),\n",
    "    (male_31_fyn_2, \"male_31_fyn.npy\"),\n",
    "    (male_55_storkoebenhavn, \"male_55_storkoebenhavn.npy\"),\n",
    "]\n",
    "\n",
    "speaker_embeddings_list_2 = [\n",
    "    (\n",
    "        create_speaker_embedding(\n",
    "            mega_enhance_audio(\n",
    "                speaker[\"audio\"][\"array\"], speaker[\"audio\"][\"sampling_rate\"]\n",
    "            )[0]\n",
    "        ),\n",
    "        file_name,\n",
    "    )\n",
    "    for (speaker, file_name) in speaker_list_2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root_metricgan_plus = \"./embeddings/nst-da-metricgan-plus/\"\n",
    "root_resemble_enhance = \"./embeddings/nst-da-resemble-enhance/\"\n",
    "Path(root_metricgan_plus).mkdir(parents=True, exist_ok=True)\n",
    "Path(root_resemble_enhance).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for embedding, file_name in speaker_embeddings_list:\n",
    "    np.save(root_metricgan_plus + file_name, embedding)\n",
    "\n",
    "for embedding, file_name in speaker_embeddings_list_2:\n",
    "    np.save(root_resemble_enhance + file_name, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example, enhance=False):\n",
    "    audio = example[\"audio\"][\"array\"]\n",
    "    if enhance:\n",
    "        audio = enhance_audio(audio)\n",
    "\n",
    "    example = processor(\n",
    "        text=example[\"text\"],\n",
    "        audio_target=audio,\n",
    "        sampling_rate=sampling_rate,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # strip off the batch dimension\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "\n",
    "    # use SpeechBrain to obtain x-vector\n",
    "\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_example = prepare_dataset(dataset[\"train\"][0])\n",
    "list(processed_example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_example[\"speaker_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(processed_example[\"labels\"].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length_max = 600\n",
    "\n",
    "\n",
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < input_length_max\n",
    "\n",
    "\n",
    "dataset = dataset.filter(\n",
    "    is_not_too_long,\n",
    "    input_columns=[\"input_ids\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "\n",
    "        # collate the inputs and targets into a batch\n",
    "        batch = processor.pad(\n",
    "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
    "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
    "        )\n",
    "\n",
    "        # not used during fine-tuning\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor(\n",
    "                [len(feature[\"input_values\"]) for feature in label_features]\n",
    "            )\n",
    "            target_lengths = target_lengths.new(\n",
    "                [\n",
    "                    length - length % model.config.reduction_factor\n",
    "                    for length in target_lengths\n",
    "                ]\n",
    "            )\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "\n",
    "        # also add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TTSDataCollatorWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5ForTextToSpeech\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(model.generate, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "finetuned_model_name = f\"{model_name}-finetuned-nst-da\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=finetuned_model_name,\n",
    "    per_device_train_batch_size=32,\n",
    "    auto_find_batch_size=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=5,\n",
    "    save_total_limit=1,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE evaluate explicitly so that best model performance is updated on model card\n",
    "trainer.evaluate(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"JackismyShephard/nst-da-norm\",\n",
    "    \"dataset\": \"NST Danish ASR Database Normalized\",\n",
    "    \"model_name\": finetuned_model_name,\n",
    "    \"finetuned_from\": checkpoint,\n",
    "    \"tasks\": \"text-to-speech\",\n",
    "    \"language\": \"da\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-to-speech\",\n",
    "    model=\"JackismyShephard/speecht5_tts-finetuned-nst-da\",\n",
    "    use_fast=True,\n",
    "    device=0 if torch.cuda.is_available() else \"cpu\",\n",
    "    revision=\"main\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_danish_letters(text):\n",
    "    for src, dst in replacements:\n",
    "        text = text.replace(src, dst)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I sin oprindelige før-kristne form blev alferne sandsynligvis opfattet som en personificering af det land og den natur, der omgav menneskene, dvs. den opdyrkede jord, gården og de naturressourcer, som hørte dertil. De var guddommelige eller delvis guddommelige væsener, der besad magiske kræfter, som de brugte både til fordel og ulempe for menneskene.\"\n",
    "text = replace_danish_letters(text)\n",
    "text2 = \"Selvom mosser er almindelige, er en lang række mosser dog knyttet snævert til habitater der er i stærk tilbagegang og disse mosser er truede. Kendtest af disse er måske tørvemosserne hvor flere arter kun forekommer i såkaldte højmoser der nu er meget sjældne i Danmark.\"\n",
    "text2 = replace_danish_letters(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./embeddings/nst-da-metricgan-plus/\"\n",
    "speaker_embedding_path = root + \"male_51_vest_sydsjaelland.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "speaker_embedding = np.load(speaker_embedding_path)\n",
    "speaker_embedding = torch.tensor(speaker_embedding).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_params = {\"speaker_embeddings\": speaker_embedding}\n",
    "speech_pipe = pipe(text2, forward_params=forward_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_enhanced, new_sr = mega_enhance_audio(\n",
    "    speech_pipe[\"audio\"], speech_pipe[\"sampling_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(speech_pipe[\"audio\"], rate=speech_pipe[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(speech_enhanced, rate=new_sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
