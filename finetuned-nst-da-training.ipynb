{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "# !pip install wheel\n",
    "# !pip install setuptools\n",
    "# !pip3 install torch\n",
    "# !pip install tensorboard\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install datasets[audio]\n",
    "# !pip install speechbrain\n",
    "# !pip install resemble-enhance\n",
    "# !pip install Cython\n",
    "# !pip install pesq\n",
    "# !pip install peft\n",
    "# !pip install evaluate\n",
    "# !pip install jiwer\n",
    "# !pip install bitsandbytes\n",
    "\n",
    "# installation of nvidia apex (also requires cuda toolkit to be installed system wide)\n",
    "# !git clone https://github.com/NVIDIA/apex\n",
    "# !cd apex\n",
    "# !pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# hf_home_dir = \"./hf_cache\"\n",
    "# os.environ[\"HF_HOME\"] = hf_home_dir  # TODO outcomment this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"alexandrainst/nst-da\")\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_train_samples = int(10000 * 0.9)\n",
    "# num_test_samples = int(10000 * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# random.seed(10)\n",
    "# train_rands = random.sample(range(len(dataset['train'])), num_train_samples)\n",
    "# dataset['train'] = dataset['train'].select(train_rands)\n",
    "\n",
    "# test_rands = random.sample(range(len(dataset['test'])), num_test_samples)\n",
    "# dataset['test'] = dataset['test'].select(test_rands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)\n",
    "tokenizer = processor.tokenizer\n",
    "processor_sr = processor.feature_extractor.sampling_rate\n",
    "assert processor_sr == 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(\n",
    "    lambda x: not (set(\"0123456789\") & set(x)), input_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "    (\"&\", \"og\"),\n",
    "    (\"\\r\", \" \"),\n",
    "    (\"´\", \"\"),\n",
    "    (\"\\\\\", \"\"),\n",
    "    (\"¨\", \" \"),\n",
    "    (\"Å\", \"AA\"),\n",
    "    (\"Æ\", \"AE\"),\n",
    "    (\"É\", \"E\"),\n",
    "    (\"Ö\", \"OE\"),\n",
    "    (\"Ø\", \"OE\"),\n",
    "    (\"á\", \"a\"),\n",
    "    (\"ä\", \"ae\"),\n",
    "    (\"å\", \"aa\"),\n",
    "    (\"è\", \"e\"),\n",
    "    (\"î\", \"i\"),\n",
    "    (\"ô\", \"oe\"),\n",
    "    (\"ö\", \"oe\"),\n",
    "    (\"ø\", \"oe\"),\n",
    "    (\"ü\", \"y\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(inputs):\n",
    "    for src, dst in replacements:\n",
    "        inputs[\"text\"] = inputs[\"text\"].replace(src, dst)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(cleanup_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "speaker_counts = defaultdict(int)\n",
    "\n",
    "for speaker_id in dataset[\"train\"][\"speaker_id\"]:\n",
    "    speaker_counts[speaker_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(speaker_counts.values(), bins=20)\n",
    "plt.ylabel(\"Speakers\")\n",
    "plt.xlabel(\"Examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_speaker(speaker_id):\n",
    "    return 280 <= speaker_counts[speaker_id] <= 327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].filter(\n",
    "    select_speaker,\n",
    "    input_columns=[\"speaker_id\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(dataset[\"train\"][\"speaker_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import SpectralMaskEnhancement\n",
    "\n",
    "metricgan_model_name = \"speechbrain/metricgan-plus-voicebank\"\n",
    "\n",
    "\n",
    "enhance_model = SpectralMaskEnhancement.from_hparams(\n",
    "    source=metricgan_model_name,\n",
    "    savedir=os.path.join(\"/tmp\", metricgan_model_name),\n",
    "    run_opts={\"device\": device},\n",
    ")\n",
    "\n",
    "\n",
    "def enhance_audio(waveform):\n",
    "    tensor = torch.tensor(waveform).reshape(1, -1).float()\n",
    "    enhanced = enhance_model.enhance_batch(tensor, lengths=torch.tensor([1.0]))\n",
    "    enhanced = enhanced.squeeze().cpu().numpy()\n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example, enhance=False):  # TODO change enhance default to False?\n",
    "    audio = example[\"audio\"][\"array\"]\n",
    "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "    if enhance:\n",
    "        audio = enhance_audio(audio)\n",
    "\n",
    "    example = processor(\n",
    "        text=example[\"text\"],\n",
    "        audio_target=audio,\n",
    "        sampling_rate=sampling_rate,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # strip off the batch dimension\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "\n",
    "    # use SpeechBrain to obtain x-vector\n",
    "\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_example = prepare_dataset(dataset[\"train\"][0])\n",
    "list(processed_example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_example[\"speaker_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(processed_example[\"labels\"].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length_max = 600\n",
    "\n",
    "\n",
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < input_length_max\n",
    "\n",
    "\n",
    "dataset = dataset.filter(\n",
    "    is_not_too_long,\n",
    "    input_columns=[\"input_ids\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "\n",
    "        # collate the inputs and targets into a batch\n",
    "        batch = processor.pad(\n",
    "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
    "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
    "        )\n",
    "\n",
    "        # not used during fine-tuning\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor(\n",
    "                [len(feature[\"input_values\"]) for feature in label_features]\n",
    "            )\n",
    "            target_lengths = target_lengths.new(\n",
    "                [\n",
    "                    length - length % model.config.reduction_factor\n",
    "                    for length in target_lengths\n",
    "                ]\n",
    "            )\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "\n",
    "        # also add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TTSDataCollatorWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5ForTextToSpeech\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\n",
    "# delattr(model.config, \"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pesq import pesq_batch\n",
    "from pesq import NoUtterancesError\n",
    "from transformers import SpeechT5HifiGan\n",
    "from mel_cepstral_distance import get_metrics_mels\n",
    "\n",
    "\n",
    "def compute_mse(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    padding_mask = label_ids != -100.0\n",
    "    pred_ids = pred_ids[padding_mask]\n",
    "    label_ids = label_ids[padding_mask]\n",
    "    assert pred_ids.shape == label_ids.shape\n",
    "    mse = ((pred_ids - label_ids) ** 2).mean()\n",
    "    return {\"mse\": mse}\n",
    "\n",
    "\n",
    "def compute_mae(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    padding_mask = label_ids != -100.0\n",
    "    pred_ids = pred_ids[padding_mask]\n",
    "    label_ids = label_ids[padding_mask]\n",
    "    assert pred_ids.shape == label_ids.shape\n",
    "    mae = np.absolute(pred_ids - label_ids).mean()\n",
    "    return {\"mae\": mae}\n",
    "\n",
    "\n",
    "def compute_mcd(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    padding_mask = label_ids[:, :, 0] != -100.0\n",
    "    mcd = 0\n",
    "    for i in range(pred_ids.shape[0]):\n",
    "        pred_ids_series = pred_ids[i]\n",
    "        label_ids_series = label_ids[i]\n",
    "        pred_ids_series = pred_ids_series[padding_mask[i]]\n",
    "        label_ids_series = label_ids_series[padding_mask[i]]\n",
    "        dist, penalty, _ = get_metrics_mels(\n",
    "            pred_ids_series, label_ids_series, take_log=False\n",
    "        )\n",
    "        mcd += dist + penalty\n",
    "\n",
    "    mcd /= pred_ids.shape[0]\n",
    "    return {\"mcd\": mcd}\n",
    "\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\n",
    "    \"microsoft/speecht5_hifigan\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "vocoder = torch.compile(vocoder)\n",
    "\n",
    "\n",
    "def compute_pesq(pred):\n",
    "\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    assert pred_ids.shape == label_ids.shape\n",
    "\n",
    "    pred_tensors = torch.tensor(pred_ids, device=\"cpu\")\n",
    "    label_tensors = torch.tensor(label_ids, device=\"cpu\")\n",
    "    pred_batches = torch.split(pred_tensors, 32)\n",
    "    label_batches = torch.split(label_tensors, 32)\n",
    "    pred_waves = []\n",
    "    label_waves = []\n",
    "    for preds, labels in zip(pred_batches, label_batches):\n",
    "        preds_gpu = preds.to(vocoder.device, vocoder.dtype)\n",
    "        labels_gpu = labels.to(vocoder.device, vocoder.dtype)\n",
    "        with torch.no_grad():\n",
    "            pred_waves.append(vocoder(preds_gpu).cpu().numpy())\n",
    "            label_waves.append(vocoder(labels_gpu).cpu().numpy())\n",
    "    pred_waves = np.concatenate(pred_waves)\n",
    "    label_waves = np.concatenate(label_waves)\n",
    "    pesq_list = pesq_batch(\n",
    "        processor_sr,\n",
    "        label_waves,\n",
    "        pred_waves,\n",
    "        mode=\"wb\",\n",
    "    )\n",
    "    pesq_metric = np.array(\n",
    "        [x for x in pesq_list if type(x) != NoUtterancesError]\n",
    "    ).mean()\n",
    "\n",
    "    return {\"pesq\": pesq_metric}\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    mse = compute_mse(pred)[\"mse\"]\n",
    "    mcd = compute_mcd(pred)[\"mcd\"]\n",
    "    mae = compute_mae(pred)[\"mae\"]\n",
    "    return {\n",
    "        \"mae\": mae,\n",
    "        \"mse\": mse,\n",
    "        \"mcd\": mcd,\n",
    "        \"mae_rmcd\": (mae + mcd * 0.010) / 2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = checkpoint.split(\"/\")[-1]\n",
    "finetuned_model_name = f\"{model_name}-finetuned-nst-da\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=finetuned_model_name,\n",
    "    per_device_train_batch_size=16,\n",
    "    dataloader_num_workers=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    num_train_epochs=train_epochs,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
    "    # fp16=True,\n",
    "    bf16=True,\n",
    "    optim=\"adamw_apex_fused\",\n",
    "    # logging_strategy= \"epoch\",\n",
    "    # log_level = \"critical\",\n",
    "    logging_steps=25,\n",
    "    # disable_tqdm = True,\n",
    "    # torch_compile=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"mcd\",\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "train_output = trainer.train(\n",
    "    resume_from_checkpoint=False,\n",
    "    ignore_keys_for_eval=[\n",
    "        \"loss\",\n",
    "        \"past_key_values\",\n",
    "        \"decoder_hidden_states\",\n",
    "        \"decoder_attentions\",\n",
    "        \"cross_attentions\",\n",
    "        \"encoder_last_hidden_state\",\n",
    "        \"encoder_hidden_states\",\n",
    "        \"encoder_attentions\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE evaluate explicitly so that best model performance is updated on model card\n",
    "eval_output = trainer.evaluate(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"alexandrainst/nst-da\",\n",
    "    \"dataset\": \"NST Danish ASR Database\",\n",
    "    \"model_name\": finetuned_model_name,\n",
    "    \"finetuned_from\": checkpoint,\n",
    "    \"tasks\": \"text-to-speech\",\n",
    "    \"language\": \"da\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_info = trainer.push_to_hub(\n",
    "    commit_message=\"train without enhancement and with mcd as evaluation metric\",\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-to-speech\",\n",
    "    model=\"JackismyShephard/speecht5_tts-finetuned-nst-da\",\n",
    "    use_fast=True,\n",
    "    device=0 if torch.cuda.is_available() else \"cpu\",\n",
    "    revision=\"fa47afb7c21c97bb7b7328a330a32898fcb215b8\",\n",
    ")\n",
    "\n",
    "pipe2 = pipeline(\n",
    "    \"text-to-speech\",\n",
    "    model=\"JackismyShephard/speecht5_tts-finetuned-nst-da\",\n",
    "    use_fast=True,\n",
    "    device=0 if torch.cuda.is_available() else \"cpu\",\n",
    "    revision=\"080e9d6dec5ac0e7f24744515955e2ab95b3a683\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "    (\"&\", \"og\"),\n",
    "    (\"\\r\", \" \"),\n",
    "    (\"´\", \"\"),\n",
    "    (\"\\\\\", \"\"),\n",
    "    (\"¨\", \" \"),\n",
    "    (\"Å\", \"AA\"),\n",
    "    (\"Æ\", \"AE\"),\n",
    "    (\"É\", \"E\"),\n",
    "    (\"Ö\", \"OE\"),\n",
    "    (\"Ø\", \"OE\"),\n",
    "    (\"á\", \"a\"),\n",
    "    (\"ä\", \"ae\"),\n",
    "    (\"å\", \"aa\"),\n",
    "    (\"è\", \"e\"),\n",
    "    (\"î\", \"i\"),\n",
    "    (\"ô\", \"oe\"),\n",
    "    (\"ö\", \"oe\"),\n",
    "    (\"ø\", \"oe\"),\n",
    "    (\"ü\", \"y\"),\n",
    "]\n",
    "\n",
    "\n",
    "def replace_danish_letters(text):\n",
    "    for src, dst in replacements:\n",
    "        text = text.replace(src, dst)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I sin oprindelige før-kristne form blev alferne sandsynligvis opfattet som en personificering af det land og den natur, der omgav menneskene, dvs. den opdyrkede jord, gården og de naturressourcer, som hørte dertil. De var guddommelige eller delvis guddommelige væsener, der besad magiske kræfter, som de brugte både til fordel og ulempe for menneskene.\"\n",
    "text = replace_danish_letters(text)\n",
    "text2 = \"Selvom mosser er almindelige, er en lang række mosser dog knyttet snævert til habitater der er i stærk tilbagegang og disse mosser er truede. Kendtest af disse er måske tørvemosserne hvor flere arter kun forekommer i såkaldte højmoser der nu er meget sjældne i Danmark.\"\n",
    "text2 = replace_danish_letters(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./embeddings/nst-da-resemble-enhance/\"\n",
    "speaker_embedding_path = root + \"female_22_oestjylland.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "speaker_embedding = np.load(speaker_embedding_path)\n",
    "speaker_embedding = torch.tensor(speaker_embedding).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_params = {\"speaker_embeddings\": speaker_embedding}\n",
    "speech_pipe = pipe(text, forward_params=forward_params)\n",
    "speech_pipe2 = pipe2(text, forward_params=forward_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resemble_enhance.enhancer.inference import enhance\n",
    "from IPython.utils import io\n",
    "\n",
    "\n",
    "def mega_enhance_audio(\n",
    "    waveform, sr, device=\"cuda\", nfe=64, solver=\"midpoint\", lambd=0.9, tau=0.95\n",
    "):\n",
    "    tensor = torch.tensor(waveform).float()\n",
    "    with io.capture_output() as _:\n",
    "        enhanced, new_sr = enhance(\n",
    "            tensor, sr, device, nfe=nfe, solver=solver, lambd=lambd, tau=tau\n",
    "        )\n",
    "    enhanced_cpu = enhanced.cpu().numpy()\n",
    "    return enhanced_cpu, new_sr\n",
    "\n",
    "\n",
    "speech_enhanced, new_sr = mega_enhance_audio(\n",
    "    speech_pipe[\"audio\"], speech_pipe[\"sampling_rate\"]\n",
    ")\n",
    "speech_enhanced2, new_sr2 = mega_enhance_audio(\n",
    "    speech_pipe2[\"audio\"], speech_pipe2[\"sampling_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(speech_pipe[\"audio\"], rate=speech_pipe[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(speech_pipe2[\"audio\"], rate=speech_pipe2[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(speech_enhanced, rate=new_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(speech_enhanced2, rate=new_sr2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
