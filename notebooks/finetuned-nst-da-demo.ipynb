{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/cdt/repositories/hugging-face-audio-course/.venv/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "checkpoint_finetuned = \"JackismyShephard/speecht5_tts-finetuned-nst-da\"\n",
    "\n",
    "revision = \"5af228df418092b681cf31c31e413bdd2b5f9c8c\"\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-to-speech\",\n",
    "    model=checkpoint_finetuned,\n",
    "    use_fast=True,\n",
    "    device=device,\n",
    "    revision=revision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = \"../embeddings/nst-da-metricgan-plus/\"\n",
    "\n",
    "speaker_embeddings = {\n",
    "    \"F23\": embeddings_dir + \"female_23_vestjylland.npy\",\n",
    "    \"F24\": embeddings_dir + \"female_24_storkoebenhavn.npy\",\n",
    "    \"F49\": embeddings_dir + \"female_49_nordjylland.npy\",\n",
    "    \"M51\": embeddings_dir + \"male_51_vest_sydsjaelland.npy\",\n",
    "    \"M18\": embeddings_dir + \"male_18_vest_sydsjaelland.npy\",\n",
    "    \"M31\": embeddings_dir + \"male_31_fyn.npy\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dtype = np.int16\n",
    "max_range = np.iinfo(target_dtype).max\n",
    "\n",
    "\n",
    "def replace_danish_letters(text):\n",
    "    for src, dst in replacements:\n",
    "        text = text.replace(src, dst)\n",
    "    return text\n",
    "\n",
    "\n",
    "replacements = [\n",
    "    (\"&\", \"og\"),\n",
    "    (\"\\r\", \" \"),\n",
    "    (\"¬¥\", \"\"),\n",
    "    (\"\\\\\", \"\"),\n",
    "    (\"¬®\", \" \"),\n",
    "    (\"√Ö\", \"AA\"),\n",
    "    (\"√Ü\", \"AE\"),\n",
    "    (\"√â\", \"E\"),\n",
    "    (\"√ñ\", \"OE\"),\n",
    "    (\"√ò\", \"OE\"),\n",
    "    (\"√°\", \"a\"),\n",
    "    (\"√§\", \"ae\"),\n",
    "    (\"√•\", \"aa\"),\n",
    "    (\"√®\", \"e\"),\n",
    "    (\"√Æ\", \"i\"),\n",
    "    (\"√¥\", \"oe\"),\n",
    "    (\"√∂\", \"oe\"),\n",
    "    (\"√∏\", \"oe\"),\n",
    "    (\"√º\", \"y\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import SpectralMaskEnhancement\n",
    "\n",
    "metricgan_model_name = \"speechbrain/metricgan-plus-voicebank\"\n",
    "\n",
    "\n",
    "enhance_model = SpectralMaskEnhancement.from_hparams(\n",
    "    source=metricgan_model_name,\n",
    "    savedir=os.path.join(\"/tmp\", metricgan_model_name),\n",
    "    run_opts={\"device\": device},\n",
    ")\n",
    "\n",
    "\n",
    "def enhance_audio(waveform):\n",
    "    tensor = torch.tensor(waveform).reshape(1, -1).float()\n",
    "    enhanced = enhance_model.enhance_batch(tensor, lengths=torch.tensor([1.0]))\n",
    "    enhanced = enhanced.squeeze().cpu().numpy()\n",
    "    return enhanced\n",
    "\n",
    "\n",
    "def enhance_audio_file(file):\n",
    "    enhanced = enhance_model.enhance_file(file)\n",
    "    enhanced = enhanced.squeeze().cpu().numpy()\n",
    "    return enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, speaker, audio=None):\n",
    "    if len(text.strip()) == 0:\n",
    "        return (16000, np.zeros(0))\n",
    "\n",
    "    text = replace_danish_letters(text)\n",
    "    if audio:\n",
    "        speaker_embedding = create_speaker_embedding(enhance_audio_file(audio))\n",
    "    else:\n",
    "        speaker_id = speaker[:3]\n",
    "\n",
    "        speaker_embedding_path = speaker_embeddings[speaker_id]\n",
    "\n",
    "        speaker_embedding = np.load(speaker_embedding_path)\n",
    "\n",
    "    speaker_embedding = torch.tensor(speaker_embedding).unsqueeze(0)\n",
    "\n",
    "    forward_params = {\"speaker_embeddings\": speaker_embedding}\n",
    "    speech = pipe(text, forward_params=forward_params)\n",
    "\n",
    "    sr, audio = speech[\"sampling_rate\"], speech[\"audio\"]\n",
    "\n",
    "    audio = (audio * max_range).astype(np.int16)\n",
    "\n",
    "    return sr, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache from '/home/cdt/repositories/hugging-face-audio-course/notebooks/gradio_cached_examples/16' directory. If method or examples have changed since last caching, delete this folder to clear cache.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdt/repositories/hugging-face-audio-course/.venv/lib/python3.10/site-packages/gradio/helpers.py:230: UserWarning: Examples are being cached but not all input components have example values. This may result in an exception being thrown by your function. If you do get an error while caching examples, make sure all of your inputs have example values for all of your examples or you provide default values for those particular parameters in your function.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "title = \"Danish Speech Synthesis\"\n",
    "\n",
    "description = (\n",
    "    \"Synthesize long-form danish speech from text with the click of a button! Demo uses the\"\n",
    "    f\" checkpoint [{checkpoint_finetuned}](https://huggingface.co/{checkpoint_finetuned}) and ü§ó Transformers to synthesize speech\"\n",
    "    \".\"\n",
    ")\n",
    "\n",
    "examples = [\n",
    "    [\n",
    "        \"I sin oprindelige f√∏r-kristne form blev alferne sandsynligvis opfattet som en personificering af det land og den natur, der omgav menneskene, dvs. den opdyrkede jord, g√•rden og de naturressourcer, som h√∏rte dertil. De var guddommelige eller delvis guddommelige v√¶sener, der besad magiske kr√¶fter, som de brugte b√•de til fordel og ulempe for menneskene.\",\n",
    "        \"F23 (Female, 23, Vestjylland)\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Input Text\"),\n",
    "        gr.Radio(\n",
    "            label=\"Preset speaker\",\n",
    "            choices=[\n",
    "                \"F23 (Female, 23, Vestjylland)\",\n",
    "                \"F24 (Female, 24, Stork√∏benhavn)\",\n",
    "                \"F49 (Female, 49 Nordjylland)\",\n",
    "                \"M51 (Male. 51, Vest-sydsj√¶lland)\",\n",
    "                \"M18 (Male, 18, Vest-sydj√¶lland)\",\n",
    "                \"M31 (Male, 31, Fyn)\",\n",
    "            ],\n",
    "            value=\"F23 (Female, 23, Vestjylland)\",\n",
    "        ),\n",
    "        gr.Audio(label=\"Custom speaker\", type=\"filepath\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Audio(label=\"Generated Speech\", type=\"numpy\"),\n",
    "    ],\n",
    "    title=title,\n",
    "    description=description,\n",
    "    examples=examples,\n",
    "    cache_examples=True,\n",
    "    allow_flagging=\"never\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://13f6ccb955dcc332e9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://13f6ccb955dcc332e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
